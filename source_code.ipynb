{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf1731d",
   "metadata": {},
   "source": [
    "# üöÄ Cryptocurrency Liquidity Prediction Project\n",
    "This project involves building a machine learning model to predict the liquidity level of cryptocurrencies using historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693cee47",
   "metadata": {},
   "source": [
    "## 1. üì• Data Collection\n",
    "We collect historical cryptocurrency data from CoinGecko for two dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv(\"coin_gecko_2022-03-16.csv\")\n",
    "df2 = pd.read_csv(\"coin_gecko_2022-03-17.csv\")\n",
    "\n",
    "# Tag with date and merge\n",
    "df1[\"date\"] = \"2022-03-16\"\n",
    "df2[\"date\"] = \"2022-03-17\"\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41c798",
   "metadata": {},
   "source": [
    "## 2. üßπ Data Preprocessing\n",
    "We clean the data by handling missing values and converting data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6979af",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"price\", \"1h\", \"24h\", \"7d\", \"24h_volume\", \"mkt_cap\"]\n",
    "df = df.dropna()\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df = df.dropna(subset=numeric_cols)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6942e2",
   "metadata": {},
   "source": [
    "## 3. üìä Exploratory Data Analysis (EDA)\n",
    "We explore data trends, correlations, and distributions to understand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ffddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Volume trend\n",
    "top_volume = df.groupby(\"coin\")[\"24h_volume\"].mean().nlargest(10)\n",
    "sns.barplot(x=top_volume.index, y=top_volume.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Coins by Avg 24h Volume\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92925a51",
   "metadata": {},
   "source": [
    "## 4. üõ†Ô∏è Feature Engineering\n",
    "We derive new features to better capture market behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df[numeric_cols])\n",
    "scaled_df = pd.DataFrame(scaled, columns=[f\"{col}_scaled\" for col in numeric_cols])\n",
    "df = pd.concat([df, scaled_df], axis=1)\n",
    "\n",
    "# New features\n",
    "df[\"volume_to_mktcap\"] = df[\"24h_volume\"] / df[\"mkt_cap\"]\n",
    "df[\"volatility_24h\"] = df[\"24h\"].abs()\n",
    "df[\"volatility_7d\"] = df[\"7d\"].abs()\n",
    "\n",
    "# Scale new features\n",
    "new_feats = [\"volume_to_mktcap\", \"volatility_24h\", \"volatility_7d\"]\n",
    "scaled_new = scaler.fit_transform(df[new_feats])\n",
    "scaled_new_df = pd.DataFrame(scaled_new, columns=[f\"{col}_scaled\" for col in new_feats])\n",
    "df = pd.concat([df, scaled_new_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa8095",
   "metadata": {},
   "source": [
    "## 5. ü§ñ Model Selection\n",
    "## 6. üèãÔ∏è Model Training\n",
    "We choose Random Forest for its robustness and train on the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3eb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = df[[\n",
    "    \"price_scaled\", \"1h_scaled\", \"24h_scaled\", \"7d_scaled\",\n",
    "    \"24h_volume_scaled\", \"mkt_cap_scaled\",\n",
    "    \"volatility_24h_scaled\", \"volatility_7d_scaled\"\n",
    "]]\n",
    "y = df[\"volume_to_mktcap\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33aa4b",
   "metadata": {},
   "source": [
    "## 7. üìà Model Evaluation\n",
    "We evaluate the model performance using standard regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe20987",
   "metadata": {},
   "source": [
    "## 8. üîß Hyperparameter Tuning\n",
    "We try another Random Forest configuration manually to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ac19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "print(\"Model 2 R2:\", r2_score(y_test, y_pred2))\n",
    "print(\"Model 2 MAE:\", mean_absolute_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f017da",
   "metadata": {},
   "source": [
    "## 9. üß™ Model Testing & Validation\n",
    "We compare actual vs predicted liquidity levels to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c80cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"Actual\": y_test.values, \"Predicted\": y_pred})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd107aa",
   "metadata": {},
   "source": [
    "## 10. üöÄ Local Deployment Preparation\n",
    "We save the trained model and scaler to use in a Streamlit web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"rf_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
